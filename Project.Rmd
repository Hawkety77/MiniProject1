---
title: "Mini Project 1"
output: pdf_document
---

```{r, include=FALSE}
df <- vroom::vroom('data/fraud.csv', col_names = FALSE)
df_transformed <- df
```
# Abstract
# Introduction
In 2024, $12.5 billion dollars were lost to fraud in the United States alone, according to the Federal Trade Commission (FTC). This staggering figure underscores the critical need for effective fraud detection mechanisms. In this analysis, we will use a dataset containing 250,000 financial transactions over a two-day period to try and identify 250 of the <500 fraudulent transactions. To identify these transactions, we will use outlier detection methods under a multivariate normal distribution assumption. 

# Data Description
The dataset consists of 250,000 observations and 12 variables. Due to the sensitive nature of the data, the variables are anonymized and labeled as X1 to X12. Each observation represents a financial transaction, with the variables capturing various attributes of these transactions. This dataset is *not labeled*, meaning we do not have prior knowledge of which transactions are fraudulent. This lack of labels necessitates the use of unsupervised learning techniques for fraud detection.

# Assessment of Normality
```{r, echo = FALSE}
# histograms for each variable par(mfrow=c(3,4))
par(mfrow=c(3,4))
for (i in 1:ncol(df)) {
  hist(df[[i]], main=paste("Histogram of X", i, sep=""), xlab=paste("X", i, sep=""), col="lightblue", border="black", breaks = 200)
}
```
An initial assessment of the data reveals that the variables do not follow a normal distribution, as evidenced by the histograms above. In general, the distributions tend to be very heavy tailed and skewed. To improve the skewness of each variable individually, we will apply a Yeo-Johnson transformation to each variable.

The Yeo-Johnson transformation is a power transformation that can handle both positive and negative values, making it more suitable for our dataset than a Box-Cox transformation. The Yeo-Johnson transformation is defined as follows:
$$
Y(\lambda) = \begin{cases}
\frac{(y + 1)^\lambda - 1}{\lambda} & \text{if } y \geq 0, \lambda \neq 0 \\ 
\log(y + 1) & \text{if } y \geq 0, \lambda = 0 \\
\frac{-(|y| + 1)^{2 - \lambda} - 1}{2 - \lambda} & \text{if } y < 0, \lambda \neq 2 \\
-\log(|y| + 1) & \text{if } y < 0, \lambda = 2
\end{cases}
$$
To choose a lambda for an optimal transformation of each variable, the log likelihood under a normal distribution is maximized with respect to lambda. To improve the normality of the distributions, we will apply the Yeo-Johnson transformation to each variable and then reassess the normality using histograms and QQ plots with confidence intervals.
```{r, echo = FALSE}
library(MASS)

# Yeo-Johnson transformation function
yeo_johnson <- function(y) {
  m <- lm(y ~ 1)
  yj_fit <- powerTransform(m, family = "yjPower")
  lambda_best <- yj_fit$lambda
  y_trans <- yjPower(y, lambda_best)
  return(list(y = y_trans, lambda = lambda_best))
}

qq_plot_with_ci <- function(x, conf = 0.95, n_highlight = 500) {
  n <- length(x)
  x_sorted <- sort(x)
  
  p <- ppoints(n)
  q_theoretical <- qnorm(p)
  q_sample <- x_sorted
  
  # Confidence bands
  alpha <- 1 - conf
  z <- qnorm(1 - alpha / 2)
  se <- (sqrt(p * (1 - p) / n)) / dnorm(q_theoretical)
  upper <- q_theoretical + z * se
  lower <- q_theoretical - z * se
  
  # Distance from y=x
  distance <- abs(q_sample - q_theoretical)
  highlight_idx <- order(distance, decreasing = TRUE)[1:min(n_highlight, n)]
  
  # Plot
  plot(q_theoretical, q_sample, main = paste0("QQ Plot with ", conf*100, "% CI"),
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", pch = 19, col = "grey20")
  abline(0, 1, col = "red", lwd = 2)
  lines(q_theoretical, upper, col = "darkgreen", lty = 2)
  lines(q_theoretical, lower, col = "darkgreen", lty = 2)
  
  # Highlight the furthest points
  points(q_theoretical[highlight_idx], q_sample[highlight_idx], pch = 19, col = "blue")
}

# Apply Yeo-Johnson transformation to each variable and save lambda to a vector
lambda_vect <- c()
for (i in 1:ncol(df)) {
  yeo <- yeo_johnson(df[[i]])
  df_transformed[[i]] <- yeo$y
  lambda_vect[i] <- yeo$lambda
}

# histogram and qq plot before and after transformation
par(mfrow=c(2,2))
hist(df$X1, main="Histogram of X1 (Before)", xlab="X1", col="lightblue", border="black", breaks = 200)
qq_plot_with_ci(df$X1, n_highlight = 500)
hist(df_transformed$X1, main="Histogram of X1 (After)", xlab="X1", col="lightblue", border="black", breaks = 200)
qq_plot_with_ci(df_transformed$X1, n_highlight = 500)
```
The figure above shows a histogram and qq plot of X1 before and after the Yeo-Johnson transformation. Initially, X1 exhibited right skewness and very heavy tails. After applying the Yeo-Johnson transformation, the distribution of X1 appears more symmetric and less heavy-tailed. The qq plot indicates that while the transformation has improved the fit to a normal distribution, there are still deviations from normality, particularly in the tails. While we would consider this transformation a step toward normality, it is still not perfectly normal. 

Each of the other variables exhibited similar improvements in normality after the Yeo-Johnson transformation, but none achieved perfect normality. The heavy-tailed nature of the distributions persists to some extent across all variables. Their lambda values are recorded in the table below:

```{r}
lambda_df <- data.frame(Variable = paste0("X", 1:ncol(df)), Lambda = lambda_vect)
knitr::kable(lambda_df, caption = "Yeo-Johnson Lambda Values for Each Variable")
```


To further assess the multivariate normality of the dataset, we will use Mahalanobis distance-based Q-Q plots. The Mahalanobis distance measures how far each observation is from the mean of the distribution, taking into account the correlations between variables. If the data were multivariate normal, the squared Mahalanobis distances would follow a Chi-square distribution with degrees of freedom equal to the number of variables. Mahalanobis distance is defined as follows:

$$
D^2 = (x - \mu)^T S^{-1} (x - \mu) \\
\text{ where } \mu \text{ is the mean vector and } S \text{ is the covariance matrix.}
$$

```{r, echo = FALSE}
mahal_plot_normal <- function(df, n_points = 500, cex = .3, xmax = NULL, ymax = NULL, name = "") {
  p <- ncol(df)
  n <- nrow(df)
  
  mu <- colMeans(df)
  S <- cov(df)
  
  # squared Mahalanobis distance
  D2 <- mahalanobis(df, mu, S)
  
  # theoretical quantiles from Chi-square distribution
  theoretical <- qchisq(ppoints(n), df = p)
  
  if (is.null(xmax)) 
    xlim <- c(0, max(theoretical))
  else
    xlim <- c(0, xmax)
  
  if (is.null(ymax)) 
    ylim <- c(0, max(D2))
  else
    ylim <- c(0, ymax)
  
  qq <- qqplot(theoretical, D2,
               xlab = "Theoretical Quantiles (Chi-sq)",
               ylab = "Sample Mahalanobis Distances",
               main = paste("Chi-sq Q-Q Plot", name),
               cex = cex,
               xlim = xlim,
               ylim = ylim
  )
  abline(0, 1, col = "red")
  
  # highlight the largest distances
  idx <- (length(qq$x) - (n_points - 1)):length(qq$x)
  points(qq$x[idx], qq$y[idx], col = "blue", pch = 19, cex = cex)
}
par(mfrow=c(1,2))
mahal_plot_normal(df, n_points = 500, ymax = 300, name = "Original")
mahal_plot_normal(df_transformed, n_points = 500, ymax = 300, name = "Transformed")
```


we will proceed with our analysis under the assumption of multivariate normality, acknowledging that the heavy-tailed nature of the distributions may lead to a higher rate of false positives in our outlier detection.

# Outlier Detection (Potential Fraud Identification)

# Discussion

# Conclusion






